{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samhitha999/BINARY-IMAGE-CLASSIFICATION-ON-LIDC-IDRI-USING-CNN-/blob/main/Binary_classify_highwayNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "-MZtdDuj5Xfw",
        "outputId": "69ef4d7e-6804-4716-886e-9e1ceb7b817e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "dir = '/content/drive/MyDrive/Lidc_512/benign'\n",
        "for image in os.listdir(dir):\n",
        "  img = cv2.imread(os.path.join(dir,image))\n",
        "  t = img.shape\n",
        "  if (t[0] != 128 or t[1] != 128 or t[2] != 3):\n",
        "    print(image, t)"
      ],
      "metadata": {
        "id": "_tECOWRjl6oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Lidc_512/malignent'\n",
        "for image in os.listdir(dir):\n",
        "  img = cv2.imread(os.path.join(dir,image))\n",
        "  t = img.shape\n",
        "  if (t[0] != 128 or t[1] != 128 or t[2] != 3):\n",
        "    print(image, t)"
      ],
      "metadata": {
        "id": "-0_yYZQInhCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtlfCZCF5MQA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Model\n",
        "from keras.layers import *\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "adam = Adam(learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDedyYIS5MQB"
      },
      "outputs": [],
      "source": [
        "# Normalize training and validation data in the range of 0 to 1\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPVji4R35MQC",
        "outputId": "e4fef70d-2ceb-4e64-f3f1-dbddb7fa158f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 783 images belonging to 2 classes.\n",
            "Found 261 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Read the training sample and set the batch size \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/Lidc_512/train',\n",
        "        target_size=(128,128),\n",
        "        batch_size=16,\n",
        "        seed=100,\n",
        "        color_mode= 'grayscale',\n",
        "        class_mode = 'binary',\n",
        "        shuffle=True)\n",
        "\n",
        "# Read Validation data from directory and define target size with batch size\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/Lidc_512/validation',\n",
        "        target_size=(128,128),\n",
        "        batch_size=16,\n",
        "        seed=100,\n",
        "        color_mode= 'grayscale',\n",
        "        class_mode = 'binary',\n",
        "        shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWoyNgNn5MQC"
      },
      "outputs": [],
      "source": [
        "#layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras import initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYw5B8y5MQD"
      },
      "outputs": [],
      "source": [
        "#convolutional layer\n",
        "\n",
        "#defining model\n",
        "model=Sequential()\n",
        "\n",
        "  #adding convolution layer\n",
        "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(128,128,3),padding='same', kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  # h transformation block\n",
        "\n",
        "for i in range(6):\n",
        "\n",
        "  model.add(Conv2D(32,(3,3),activation='relu',padding='same', kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(32,(3,3),activation='relu',padding='same', kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(0.01)))\n",
        "  model.add(Conv2D(32,(3,3),activation='sigmoid',padding='same',kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  #adding fully connected layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='sigmoid'))\n",
        "model.add(Dense(10, activation='sigmoid'))\n",
        "model.add(Dense(2,activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IDgYQia5MQD",
        "outputId": "deb9c391-98a8-4f9b-c1a3-bebdcbaf03d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 128, 128, 32)      896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128, 128, 32)     128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 128, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128, 128, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 128, 128, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 128, 128, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 128, 128, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 128, 128, 32)      9248      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 524288)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                33554496  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 22        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,723,296\n",
            "Trainable params: 33,722,912\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
      ],
      "metadata": {
        "id": "oyPaJmEuzjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDr84w_D5MQE"
      },
      "outputs": [],
      "source": [
        "# We are going to use accuracy metrics and cross entropy loss as performance parameters\n",
        "\n",
        "# Train the model \n",
        "#print('check: ',train_generator.samples/train_generator.batch_size)\n",
        "history = model.fit(train_generator, #object(model to train) \n",
        "      epochs=50,\n",
        "     \n",
        "      verbose=1) \n",
        "\"\"\"\n",
        "specifies verbosity mode(0 = silent, 1= progress bar, 2 = one line per epoch\n",
        "validation_data can be either:\n",
        "                      - an inputs and targets list\n",
        "                      - a generator\n",
        "                      - an inputs, targets, and sample_weights\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oY-VWBJOgkd"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/malignancy_classification_test.h5')\n",
        "\n",
        "from tensorflow.keras import models\n",
        "model = models.load_model('/content/drive/MyDrive/malignancy_classification_test.h5')\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/malignancy_classification_weights_test.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKBLdy59cInY"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = range(len(train_acc)) \n",
        "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/dataset LIDC-IDRI/clip directory/test',\n",
        "        target_size=(128,128),\n",
        "        batch_size=1,\n",
        "        seed=100,\n",
        "        color_mode= 'grayscale',\n",
        "        class_mode = None,\n",
        "        shuffle=False)"
      ],
      "metadata": {
        "id": "BgZvqr7ZDaEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = list(testdict.values())\n",
        "test_generator.reset()\n",
        "y_pred1 = model.predict(test_generator)\n",
        "\n",
        "#y_pred\n",
        "len(y_pred1)"
      ],
      "metadata": {
        "id": "XWmemLNpYIVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred1)"
      ],
      "metadata": {
        "id": "2BsuajaGEQiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "vNTc1u-n4zkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP_SIZE_TEST=int(test_generator.samples)\n",
        "# test_generator.reset()\n",
        "# y_pred=model.predict_generator(test_generator,\n",
        "# steps=1,\n",
        "# verbose=1)"
      ],
      "metadata": {
        "id": "zMqlt3rLV4GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "y_pred=[]\n",
        "for i in range(len(y_pred1)):\n",
        "  y_pred.append(int(math.ceil(y_pred1[i][1])))\n"
      ],
      "metadata": {
        "id": "M4lSaxRr-lXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "5fz-yZ5EmUGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZbbybymhCUI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "acc =accuracy_score(y_test, y_pred)\n",
        "print(acc)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "print(precision)\n",
        "\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "print(recall)\n",
        "\n",
        "f1=f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform 5 fold cross validation on your model as shown in below code."
      ],
      "metadata": {
        "id": "Wudjb38dNv58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "def built_classifier():\n",
        "    #classifier=Sequential()\n",
        "    #classifier.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=64))\n",
        "    #classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
        "    #classifier.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    #return classifier\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Convolutional layer and maxpool layer 1\n",
        "    model.add(Conv2D(24,(5,5),input_shape= input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.05))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Conv2D(32,(3,3)))\n",
        "    model.add(LeakyReLU(alpha=0.05))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Conv2D(64,(3,3)))\n",
        "    model.add(LeakyReLU(alpha=0.05))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Hidden layer with 512 neurons and Rectified Linear Unit activation function \n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.05))\n",
        "    model.add(Dense(16))\n",
        "    #model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation= 'softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "classifier = KerasClassifier(build_fn= built_classifier, batch_size = 224, epochs=80)\n",
        "accuracies= cross_val_score(estimator = classifier, X=x_train, y=y_train, cv=5, n_jobs=-1, error_score=\"raise\")\n",
        "\n",
        "accuracies\n"
      ],
      "metadata": {
        "id": "xKwoQ5yMMzbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the metrics package from sklearn library\n",
        "from sklearn import metrics\n",
        "# Creating the confusion matrix\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "# Assigning columns names\n",
        "cm_df = pd.DataFrame(cm, \n",
        "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
        "            index = ['Actual Negative', 'Actual Positive'])\n",
        "# Showing the confusion matrix\n",
        "cm_df"
      ],
      "metadata": {
        "id": "ZtRBzfP0Npd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to report confusion metrics\n",
        "def confusion_metrics (conf_matrix):\n",
        "# save confusion matrix and slice into four pieces\n",
        "    TP = conf_matrix[1][1]\n",
        "    TN = conf_matrix[0][0]\n",
        "    FP = conf_matrix[0][1]\n",
        "    FN = conf_matrix[1][0]\n",
        "    print('True Positives:', TP)\n",
        "    print('True Negatives:', TN)\n",
        "    print('False Positives:', FP)\n",
        "    print('False Negatives:', FN)\n",
        "    \n",
        "    # calculate accuracy\n",
        "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
        "    \n",
        "    # calculate mis-classification\n",
        "    conf_misclassification = 1- conf_accuracy\n",
        "    \n",
        "    # calculate the sensitivity\n",
        "    conf_sensitivity = (TP / float(TP + FN))\n",
        "    # calculate the specificity\n",
        "    conf_specificity = (TN / float(TN + FP))\n",
        "    \n",
        "    # calculate precision\n",
        "    conf_precision = (TN / float(TN + FP))\n",
        "    # calculate f_1 score\n",
        "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
        "    print('-'*50)\n",
        "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
        "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
        "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
        "    print(f'Specificity: {round(conf_specificity,2)}') \n",
        "    print(f'Precision: {round(conf_precision,2)}')\n",
        "    print(f'f_1 Score: {round(conf_f1,2)}')"
      ],
      "metadata": {
        "id": "c18LVNaPStef"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "9ddfba760c93d0781cb88c4db9a31eb68ca4e1616821530f19d735f356a5c9ec"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}